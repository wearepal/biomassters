# @package _global_

defaults:
    - override /model: unet3d_vd/spatial_128dim
    - override /dm: all_months/zscore_norm
    - override /alg: erm
    - override /logger: pal
    - override /checkpointer: bio
    - _self_

trainer:
  precision: 16
  accelerator: gpu
  devices: 4
  max_steps: ${eval:'int(3e4 * ${trainer.accumulate_grad_batches})'}
  strategy: 
    _target_: pytorch_lightning.strategies.DeepSpeedStrategy
    stage: 2
    offload_optimizer: false
    offload_parameters: false
    min_loss_scale: 1.0

  val_check_interval: ${eval:'int(5e2 * ${trainer.accumulate_grad_batches})'}
  # val_check_interval: ${eval:'1000 * ${trainer.accumulate_grad_batches}'}
  gradient_clip_val: 100

alg:
  pred_dir: /srv/galene0/shared/data/biomassters/predictions/unet3d/${EPOCHSECONDS}
  test_on_best: false
  loss_fn: 
    _target_: torch.nn.HuberLoss
    delta: 1.0
    # _target_: torch.nn.L1Loss
  # optimizer_cls: deepspeed.ops.adam.FusedAdam
  optimizer_cls: src.optimizers.Adafactor
  lr: 3.e-4
  weight_decay: 0.0
  # weight_decay: 1.e-2

  lr_sched_freq: 1
  scheduler_cls: ranzen.torch.schedulers.CosineLRWithLinearWarmup
  scheduler_kwargs:
    # warmup_iters: ${eval:'int(2e3 * ${trainer.accumulate_grad_batches})'}
    warmup_iters: ${eval:'${trainer.max_steps} // 20'}
    total_iters: ${eval:'${trainer.max_steps} // ${trainer.accumulate_grad_batches}'}
    lr_min: 5.0e-7

dm:
  num_workers: 24
  train_batch_size: 1

logger:
  group: unet3d_128dim_spatial
  tags:
    - base_dim_128
    - attn_head_dim_64
    - spatial_attn
    - spatial_decoder
    - unet3d
    - zscore_norm

