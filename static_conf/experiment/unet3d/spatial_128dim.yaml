# @package _global_

defaults:
    - override /model: unet3d_vd/spatial_128dim
    - override /dm: all_months/zscore_norm
    - override /alg: erm
    - override /logger: pal
    - override /checkpointer: bio
    - _self_


alg:
  pred_dir: /srv/galene0/shared/data/biomassters/predictions/unet3d/${EPOCHSECONDS}
  lr: 1e-4
  weight_decay: 1.e-4
  test_on_best: false
  loss_fn: null
    # _target_: torch.nn.HuberLoss
    # delta: 1.0
    # _target_: src.loss.CharbonnierLoss
    # alpha: 2
  optimizer_cls: 'deepspeed.ops.adam.FusedAdam'
  # optimizer_cls: torch.optim.AdamW
  # optimizer_cls: src.optimizers.Adafactor
  # optimizer_cls: bitsandbytes.optim.Adam8bit
  lr_sched_freq: 1
  # scheduler_cls: torch.optim.lr_scheduler.CosineAnnealingLR
  # scheduler_kwargs:
  #   T_max: ${ trainer.max_steps }
  #   eta_min: 5.0e-7
  scheduler_cls: ranzen.torch.schedulers.CosineLRWithLinearWarmup
  scheduler_kwargs:
    warmup_iters: ${eval:'2000 * ${trainer.accumulate_grad_batches}'}
    total_iters: ${eval:'${trainer.max_steps} // ${trainer.accumulate_grad_batches}'}
    lr_min: 5.0e-7

dm:
  num_workers: 24
  train_batch_size: 1

trainer:
  precision: 16
  max_steps: 30000
  val_check_interval: ${eval:'500 * ${trainer.accumulate_grad_batches}'}
  accelerator: gpu
  devices: 4
  strategy: deepspeed_stage_2

logger:
  group: unet3d_128dim_spatial
  tags:
    - base_dim_128
    - attn_head_dim_64
    - spatial_attn
    - spatial_decoder
    - unet3d
    - zscore_norm

