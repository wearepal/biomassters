# @package _global_

defaults:
    - override /model: unet3d_vd/temporal_128dim
    - override /dm: all_months/zscore_norm
    - override /alg: erm
    - override /logger: pal
    - override /checkpointer: bio
    - _self_

trainer:
  precision: 16
  max_steps: 40000
  val_check_interval: 500
  accelerator: gpu
  devices: 4
  strategy: 
    _target_: pytorch_lightning.strategies.DeepSpeedStrategy
    stage: 2
    offload_optimizer: false
    offload_parameters: false
    min_loss_scale: 1

alg:
  pred_dir: /srv/galene0/shared/data/biomassters/predictions/unet3d/${EPOCHSECONDS}
  lr: 3e-4
  weight_decay: 0.0
  lr_sched_freq: 1
  test_on_best: false
  loss_fn: 
    _target_: src.loss.CharbonnierLoss
    alpha: 2
  optimizer_cls: deepspeed.ops.adam.FusedAdam

  # scheduler_cls: torch.optim.lr_scheduler.CosineAnnealingLR
  # scheduler_kwargs:
  #   T_max: ${ trainer.max_steps }
  #   eta_min: 5.0e-7
  scheduler_cls: ranzen.torch.schedulers.CosineLRWithLinearWarmup
  scheduler_kwargs:
    warmup_iters: 1000
    total_iters: ${ trainer.max_steps }
    lr_min: 5.0e-7

dm:
  num_workers: 24
  train_batch_size: 1

logger:
  group: unet3d_128dim_temporal_decoder
  tags:
    - base_dim_128
    - attn_head_dim_64
    - spatial_attn
    - temporal_decoder
    - unet3d
    - zscore_norm

