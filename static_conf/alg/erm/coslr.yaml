---
defaults:
  - /schema/alg: erm
  - _self_
lr: 5e-05
weight_decay: 0.0
lr_sched_freq: 1
test_on_best: false

loss_fn: null
  # _target_: torch.nn.HuberLoss
  # delta: 1.0

optimizer_cls: 'torch.optim.AdamW'
optimizer_kwargs: null

# scheduler_cls: torch.optim.lr_scheduler.CosineAnnealingLR
# scheduler_kwargs:
#   T_max: ${ trainer.max_steps }
#   eta_min: 5.0e-7
scheduler_cls: ranzen.torch.schedulers.CosineLRWithLinearWarmup
scheduler_kwargs:
  warmup_iters: 1000
  total_iters: ${ trainer.max_steps }
  lr_min: 5.0e-7
